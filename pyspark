Jasne, oto gotowe rozwiÄ…zanie w PySpark, ktÃ³re moÅ¼esz uruchomiÄ‡ w Jupyter Lab, aby zrealizowaÄ‡ swoje zadanie. PoniÅ¼szy skrypt wykonuje wszystkie wymagane kroki: odnajduje najnowszy wpis dla kaÅ¼dego wniosku, parsuje dane z kolumny JSON i zapisuje wyniki do bazy danych MS SQL Server.
âœ… Gotowy skrypt PySpark
PoniÅ¼szy kod jest kompletnym rozwiÄ…zaniem. PamiÄ™taj, aby uzupeÅ‚niÄ‡ dane w sekcji KONFIGURACJA.
from pyspark.sql import SparkSession, Window
from pyspark.sql.functions import col, from_json, row_number, lit
from pyspark.sql.types import StructType, StructField, StringType

# ==============================================================================
# KONFIGURACJA - UzupeÅ‚nij poniÅ¼sze zmienne
# ==============================================================================

# Nazwa widoku w Hadoop (Hive/Impala)
HADOOP_VIEW_NAME = "twoja_baza.twoj_widok"

# SzczegÃ³Å‚y poÅ‚Ä…czenia z MS SQL Server
SQL_SERVER_HOST = "nazwa_twojego_serwera.database.windows.net"
SQL_DATABASE = "nazwa_twojej_bazy"
SQL_USER = "twoj_uzytkownik"
SQL_PASSWORD = "twoje_haslo"
SQL_TABLE_NAME = "nazwa_tabeli_docelowej" # Tabela, do ktÃ³rej zostanÄ… zapisane dane

# Sterownik JDBC - upewnij siÄ™, Å¼e jest dostÄ™pny dla Sparka
# Zazwyczaj Å›cieÅ¼kÄ™ do pliku .jar podaje siÄ™ przy starcie sesji Sparka
# np. --jars /path/to/mssql-jdbc-x.x.x.jre8.jar
JDBC_DRIVER = "com.microsoft.sqlserver.jdbc.SQLServerDriver"

# ==============================================================================
# 1. Inicjalizacja sesji Spark
# ==============================================================================

spark = SparkSession.builder \
    .appName("PrzetwarzanieWnioskowZJson") \
    .enableHiveSupport() \
    .getOrCreate()

print("âœ… Sesja Spark zostaÅ‚a pomyÅ›lnie utworzona.")

# ==============================================================================
# 2. Wczytanie danych z widoku Hadoop
# ==============================================================================

try:
    df_source = spark.table(HADOOP_VIEW_NAME)
    print(f"âœ… PomyÅ›lnie wczytano dane z widoku: {HADOOP_VIEW_NAME}")
    # Opcjonalnie: wyÅ›wietl schemat i prÃ³bkÄ™ danych
    # df_source.printSchema()
    # df_source.show(5, truncate=False)
except Exception as e:
    print(f"âŒ BÅ‚Ä…d podczas wczytywania danych z widoku: {e}")
    spark.stop()
    exit()

# ==============================================================================
# 3. Odnalezienie ostatniego wpisu dla kaÅ¼dego wniosku
# ==============================================================================

# Definiujemy "okno" partycjonowane po numerze wniosku i sortowane malejÄ…co po dacie modyfikacji
window_spec = Window.partitionBy("numer_wniosku").orderBy(col("data_modyfikacji").desc())

# Dodajemy kolumnÄ™ z numerem wiersza w danym "oknie"
# Najnowszy wpis dla kaÅ¼dego wniosku bÄ™dzie miaÅ‚ rn = 1
df_latest = df_source.withColumn("rn", row_number().over(window_spec)) \
                     .where(col("rn") == 1) \
                     .drop("rn")

print("âœ… Znaleziono najnowsze wpisy dla kaÅ¼dego wniosku.")

# ==============================================================================
# 4. Parsowanie kolumny JSON i ekstrakcja kluczy
# ==============================================================================

# Definiujemy schemat dla danych JSON, ktÃ³re chcemy wyciÄ…gnÄ…Ä‡
json_schema = StructType([
    StructField("status", StringType(), True),
    StructField("doradca", StringType(), True),
    StructField("placowka", StringType(), True)
])

# Parsujemy kolumnÄ™ 'value' (ktÃ³ra jest stringiem) do nowej kolumny 'json_data' (ktÃ³ra ma strukturÄ™)
# NastÄ™pnie wybieramy interesujÄ…ce nas pola, nadajÄ…c im nowe, czytelne nazwy
df_parsed = df_latest.withColumn("json_data", from_json(col("value"), json_schema)) \
                     .select(
                         col("numer_wniosku"),
                         col("data_modyfikacji"),
                         col("json_data.status").alias("status_wniosku"),
                         col("json_data.doradca").alias("doradca"),
                         col("json_data.placowka").alias("placowka")
                     )

print("âœ… PomyÅ›lnie sparsowano dane JSON i wyekstrahowano wartoÅ›ci.")
print("PrzykÅ‚adowe przetworzone dane:")
df_parsed.show(10, truncate=False)


# ==============================================================================
# 5. Zapis wynikÃ³w do bazy MS SQL Server
# ==============================================================================

# Budowanie URL do poÅ‚Ä…czenia JDBC
jdbc_url = f"jdbc:sqlserver://{SQL_SERVER_HOST};databaseName={SQL_DATABASE}"

# Konfiguracja wÅ‚aÅ›ciwoÅ›ci poÅ‚Ä…czenia
connection_properties = {
    "user": SQL_USER,
    "password": SQL_PASSWORD,
    "driver": JDBC_DRIVER
}

try:
    # Zapis DataFrame do tabeli w SQL Server
    # tryb 'overwrite' nadpisze tabelÄ™, 'append' doÅ‚Ä…czy nowe wiersze
    df_parsed.write.jdbc(
        url=jdbc_url,
        table=SQL_TABLE_NAME,
        mode="append", 
        properties=connection_properties
    )
    print(f"âœ… Dane zostaÅ‚y pomyÅ›lnie zapisane do tabeli '{SQL_TABLE_NAME}' w MS SQL Server.")
except Exception as e:
    print(f"âŒ WystÄ…piÅ‚ bÅ‚Ä…d podczas zapisu do MS SQL Server: {e}")

# ==============================================================================
# 6. ZakoÅ„czenie sesji Spark
# ==============================================================================

spark.stop()
print("âœ… Sesja Spark zostaÅ‚a zakoÅ„czona.")


ğŸ“œ WyjaÅ›nienie kluczowych krokÃ³w
 * Konfiguracja: Na poczÄ…tku skryptu znajdujÄ… siÄ™ wszystkie zmienne, ktÃ³re musisz dostosowaÄ‡ do swojego Å›rodowiska, takie jak nazwa widoku, dane logowania do SQL Servera i nazwa tabeli docelowej.
 * Inicjalizacja Sparka: Tworzona jest standardowa sesja SparkSession, ktÃ³ra jest punktem wejÅ›cia do pracy z danymi w Sparku. enableHiveSupport() pozwala na odczyt tabel/widokÃ³w z Hadoopa.
 * Odnalezienie ostatniego wpisu: To kluczowa czÄ™Å›Ä‡ logiki. UÅ¼ywamy tu funkcji okienkowej (Window):
   * partitionBy("numer_wniosku"): Dzieli dane na grupy, gdzie kaÅ¼da grupa zawiera wszystkie wpisy dla jednego numeru wniosku.
   * orderBy(col("data_modyfikacji").desc()): WewnÄ…trz kaÅ¼dej z tych grup, sortuje wpisy od najnowszego do najstarszego.
   * row_number(): Numeruje posortowane wiersze w kaÅ¼dej grupie. DziÄ™ki temu najnowszy wpis zawsze otrzyma numer 1.
   * .where(col("rn") == 1): Na koÅ„cu filtrujemy dane, zostawiajÄ…c tylko te wiersze, ktÃ³re majÄ… numer 1, czyli najnowsze wpisy dla kaÅ¼dego unikalnego wniosku.
 * Parsowanie JSON:
   * json_schema: Definiujemy strukturÄ™ pÃ³l, ktÃ³re chcemy "wyciÄ…gnÄ…Ä‡" z tekstu JSON. Jest to dobra praktyka, ktÃ³ra zwiÄ™ksza wydajnoÅ›Ä‡.
   * from_json(): Ta funkcja Sparka konwertuje kolumnÄ™ tekstowÄ… (value) zawierajÄ…cÄ… JSON na kolumnÄ™ o zdefiniowanej strukturze (json_data).
   * .select(): Po sparsowaniu, moÅ¼emy Å‚atwo odwoÅ‚aÄ‡ siÄ™ do poszczegÃ³lnych kluczy jak do zwykÅ‚ych kolumn (np. json_data.status) i nadaÄ‡ im nowe, przyjazne nazwy (alias).
 * Zapis do MS SQL Server:
   * Funkcja write.jdbc() pozwala na prosty zapis DataFrame do dowolnej bazy danych obsÅ‚ugujÄ…cej JDBC.
   * url: Standardowy adres poÅ‚Ä…czenia JDBC.
   * table: Nazwa tabeli docelowej.
   * mode: OkreÅ›la, co zrobiÄ‡, jeÅ›li tabela juÅ¼ istnieje. NajczÄ™Å›ciej uÅ¼ywane to append (doÅ‚Ä…cz dane) lub overwrite (usuÅ„ starÄ… tabelÄ™ i stwÃ³rz nowÄ… z tymi danymi).
   * properties: SÅ‚ownik zawierajÄ…cy dane uwierzytelniajÄ…ce.
Wymagania przed uruchomieniem
 * Sterownik JDBC: Sesja Spark musi mieÄ‡ dostÄ™p do sterownika JDBC dla MS SQL Server. Zazwyczaj plik .jar sterownika dodaje siÄ™ podczas konfiguracji klastra lub przy starcie sesji Jupyter/Spark, np. poprzez parametr --jars. Upewnij siÄ™, Å¼e odpowiednia wersja sterownika jest dostÄ™pna w Twoim Å›rodowisku.
 * Uprawnienia: UÅ¼ytkownik, ktÃ³rego danych uÅ¼ywasz do poÅ‚Ä…czenia z SQL Serverem, musi mieÄ‡ uprawnienia do zapisu (INSERT, CREATE TABLE) w docelowej bazie danych i schemacie.


OczywiÅ›cie, to bardzo czÄ™sty przypadek. ObsÅ‚uga zagnieÅ¼dÅ¼onych struktur JSON w PySpark jest prosta i intuicyjna. Wystarczy odzwierciedliÄ‡ tÄ™ zagnieÅ¼dÅ¼onÄ… strukturÄ™ w definicji schematu (StructType), a nastÄ™pnie odwoÅ‚ywaÄ‡ siÄ™ do gÅ‚Ä™bszych pÃ³l za pomocÄ… notacji kropkowej.
ZaÅ‚Ã³Å¼my, Å¼e TwÃ³j JSON w kolumnie value wyglÄ…da teraz tak, gdzie klucz dane_dodatkowe sam zawiera kolejne klucze i wartoÅ›ci:
{
  "status": "W trakcie",
  "doradca": "Anna Nowak",
  "placowka": "OddziaÅ‚ Regionalny",
  "dane_dodatkowe": {
    "typ_produktu": "Kredyt gotÃ³wkowy",
    "segment_klienta": "Premium"
  },
  "kontakt": {
      "email": "a.nowak@example.com",
      "telefon": "987654321"
  }
}

I chcesz wyciÄ…gnÄ…Ä‡ status, doradca oraz zagnieÅ¼dÅ¼one wartoÅ›ci typ_produktu i email.
PoniÅ¼ej znajduje siÄ™ zmodyfikowany skrypt, ktÃ³ry to realizuje. Zmiany zostaÅ‚y wprowadzone w krokach 4A i 4B.
âœ… Zaktualizowany skrypt PySpark (obsÅ‚uga zagnieÅ¼dÅ¼onego JSON)
from pyspark.sql import SparkSession, Window
from pyspark.sql.functions import col, from_json, row_number, lit
# Importujemy dodatkowo IntegerType, bo zagnieÅ¼dÅ¼one pola mogÄ… mieÄ‡ rÃ³Å¼ne typy
from pyspark.sql.types import StructType, StructField, StringType, IntegerType 

# ==============================================================================
# KONFIGURACJA - UzupeÅ‚nij poniÅ¼sze zmienne
# ==============================================================================

# Nazwa widoku w Hadoop (Hive/Impala)
HADOOP_VIEW_NAME = "twoja_baza.twoj_widok"

# SzczegÃ³Å‚y poÅ‚Ä…czenia z MS SQL Server
SQL_SERVER_HOST = "nazwa_twojego_serwera.database.windows.net"
SQL_DATABASE = "nazwa_twojej_bazy"
SQL_USER = "twoj_uzytkownik"
SQL_PASSWORD = "twoje_haslo"
SQL_TABLE_NAME = "nazwa_tabeli_docelowej" 

JDBC_DRIVER = "com.microsoft.sqlserver.jdbc.SQLServerDriver"

# ==============================================================================
# 1. Inicjalizacja sesji Spark
# ==============================================================================

spark = SparkSession.builder \
    .appName("PrzetwarzanieZagniezdzonychWnioskowZJson") \
    .enableHiveSupport() \
    .getOrCreate()

print("âœ… Sesja Spark zostaÅ‚a pomyÅ›lnie utworzona.")

# ==============================================================================
# 2. Wczytanie danych z widoku Hadoop
# ==============================================================================

try:
    df_source = spark.table(HADOOP_VIEW_NAME)
    print(f"âœ… PomyÅ›lnie wczytano dane z widoku: {HADOOP_VIEW_NAME}")
except Exception as e:
    print(f"âŒ BÅ‚Ä…d podczas wczytywania danych z widoku: {e}")
    spark.stop()
    exit()

# ==============================================================================
# 3. Odnalezienie ostatniego wpisu dla kaÅ¼dego wniosku
# ==============================================================================

window_spec = Window.partitionBy("numer_wniosku").orderBy(col("data_modyfikacji").desc())

df_latest = df_source.withColumn("rn", row_number().over(window_spec)) \
                     .where(col("rn") == 1) \
                     .drop("rn")

print("âœ… Znaleziono najnowsze wpisy dla kaÅ¼dego wniosku.")

# ==============================================================================
# 4. Parsowanie zagnieÅ¼dÅ¼onej kolumny JSON i ekstrakcja kluczy
# ==============================================================================

# ------------------------------------------------------------------------------
# 4A. Zdefiniuj schemat, ktÃ³ry odzwierciedla zagnieÅ¼dÅ¼onÄ… strukturÄ™ JSON
# ------------------------------------------------------------------------------
# Klucz 'dane_dodatkowe' i 'kontakt' majÄ… jako wartoÅ›Ä‡ kolejny StructType
json_schema = StructType([
    StructField("status", StringType(), True),
    StructField("doradca", StringType(), True),
    StructField("placowka", StringType(), True),
    StructField("dane_dodatkowe", StructType([
        StructField("typ_produktu", StringType(), True),
        StructField("segment_klienta", StringType(), True)
        # MoÅ¼esz tu dodaÄ‡ wiÄ™cej pÃ³l z tego zagnieÅ¼dÅ¼enia
    ]), True),
    StructField("kontakt", StructType([
        StructField("email", StringType(), True),
        StructField("telefon", StringType(), True)
    ]), True)
])

# ------------------------------------------------------------------------------
# 4B. Wybierz pola, uÅ¼ywajÄ…c notacji kropkowej do dostÄ™pu do zagnieÅ¼dÅ¼onych wartoÅ›ci
# ------------------------------------------------------------------------------
df_parsed = df_latest.withColumn("json_data", from_json(col("value"), json_schema)) \
                     .select(
                         col("numer_wniosku"),
                         col("data_modyfikacji"),
                         col("json_data.status").alias("status_wniosku"),
                         col("json_data.doradca").alias("doradca"),
                         # UÅ¼yj notacji kropkowej, aby dostaÄ‡ siÄ™ do zagnieÅ¼dÅ¼onych pÃ³l
                         col("json_data.dane_dodatkowe.typ_produktu").alias("typ_produktu"),
                         col("json_data.kontakt.email").alias("email_kontaktowy")
                     )

print("âœ… PomyÅ›lnie sparsowano zagnieÅ¼dÅ¼one dane JSON i wyekstrahowano wartoÅ›ci.")
print("PrzykÅ‚adowe przetworzone dane:")
df_parsed.show(10, truncate=False)

# ==============================================================================
# 5. Zapis wynikÃ³w do bazy MS SQL Server
# ==============================================================================

jdbc_url = f"jdbc:sqlserver://{SQL_SERVER_HOST};databaseName={SQL_DATABASE}"
connection_properties = {
    "user": SQL_USER,
    "password": SQL_PASSWORD,
    "driver": JDBC_DRIVER
}

try:
    df_parsed.write.jdbc(
        url=jdbc_url,
        table=SQL_TABLE_NAME,
        mode="append",
        properties=connection_properties
    )
    print(f"âœ… Dane zostaÅ‚y pomyÅ›lnie zapisane do tabeli '{SQL_TABLE_NAME}' w MS SQL Server.")
except Exception as e:
    print(f"âŒ WystÄ…piÅ‚ bÅ‚Ä…d podczas zapisu do MS SQL Server: {e}")

# ==============================================================================
# 6. ZakoÅ„czenie sesji Spark
# ==============================================================================

spark.stop()
print("âœ… Sesja Spark zostaÅ‚a zakoÅ„czona.")

ğŸ“œ Kluczowe zmiany i wyjaÅ›nienie
 * Definicja schematu (json_schema):
   * GÅ‚Ã³wna zmiana polega na tym, Å¼e dla kluczy, ktÃ³re same zawierajÄ… obiekt JSON (jak dane_dodatkowe i kontakt), jako typ pola (StructField) podajemy kolejny StructType.
   * W ten sposÃ³b tworzysz "drzewo" schematu, ktÃ³re idealnie pasuje do struktury Twoich danych. DziÄ™ki temu Spark wie dokÅ‚adnie, jak zinterpretowaÄ‡ dane, co jest znacznie wydajniejsze niÅ¼ zgadywanie schematu.
 * DostÄ™p do zagnieÅ¼dÅ¼onych pÃ³l (.select()):
   * Aby uzyskaÄ‡ dostÄ™p do wartoÅ›ci w zagnieÅ¼dÅ¼onym obiekcie, po prostu Å‚Ä…czysz nazwy kluczy kropkami: col("json_data.dane_dodatkowe.typ_produktu").
   * MoÅ¼esz w ten sposÃ³b schodziÄ‡ na dowolny poziom zagnieÅ¼dÅ¼enia, np. json_data.klucz1.klucz2.klucz3...
   * UÅ¼ycie alias() jest dobrÄ… praktykÄ…, aby nadaÄ‡ wynikowym kolumnom czytelne i pÅ‚askie nazwy w docelowej tabeli SQL.

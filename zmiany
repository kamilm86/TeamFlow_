
Jasne, rozumiem zadanie. Działam jako doświadczony analityk danych. Poniżej przedstawiam kompletną analizę w formacie notatnika Jupyter. Zawarłem w niej kod, wizualizacje, a co najważniejsze – interpretację biznesową wyników i rekomendacje, czyli to, czego oczekuje się od starszego analityka.
Analiza Porównawcza Połączeń z Infolinią Rok do Roku (YoY)
Data: 16 września 2025
Autor: Gemini - Analityk Danych
🎯 1. Cel Analizy
Głównym celem tej analizy jest przeprowadzenie szczegółowego porównania rocznego (Year-over-Year) kluczowych wskaźników wydajności (KPI) infolinii. Skupimy się na zrozumieniu zmian w wolumenie, efektywności i charakterystyce interakcji klientów. Analiza ma dostarczyć actionable insights, które pomogą w optymalizacji procesów, alokacji zasobów i planowaniu strategicznym.
Główne pytania badawcze:
 * Jak zmienił się ogólny wolumen interakcji rok do roku?
 * Które kanały kontaktu zyskały, a które straciły na popularności?
 * Jak ewoluowała efektywność operacyjna (np. średni czas rozmowy)?
 * Czy zmienił się sposób, w jaki klienci trafiają na infolinię (wejscie) i gdzie kończą swoją podróż (gdzie koniec)?
 * Czy obserwujemy zmiany w sezonowości lub trendach w ciągu roku?
🤔 2. Założenia i Dodatkowe Pytania
Zanim przejdziemy do kodu, jako analityk muszę jasno określić założenia i zidentyfikować potencjalne braki w danych. Twoje pytanie "czy o wszystkim napisałem" jest kluczowe.
Założenia:
 * Zakładam, że tabela zawiera dane z co najmniej dwóch pełnych, porównywalnych lat.
 * czas rozmowy to suma czasu w sekundach dla danej grupy.
 * Tabela, oprócz wymienionych kolumn, musi zawierać liczbę połączeń/interakcji dla każdego wiersza (np. w kolumnie liczba_polaczen). Bez tej metryki nie będziemy w stanie policzyć kluczowych wskaźników, jak średni czas rozmowy (AHT). Jeśli jej nie ma, jest to krytyczny brak.
Pytania doprecyzowujące (elementy, które warto dodać w przyszłości):
 * Kluczowa metryka biznesowa: Czy posiadamy dane o kosztach obsługi per kanał? To pozwoliłoby na analizę Cost per Interaction.
 * Jakość obsługi: Czy zbieramy dane CSAT/NPS (satysfakcja klienta)? Połączenie danych operacyjnych z jakościowymi daje pełen obraz sytuacji.
 * FCR (First Contact Resolution): Czy mamy flagę określającą, czy problem został rozwiązany przy pierwszym kontakcie? To jeden z najważniejszych wskaźników efektywności infolinii.
 * Szczegółowość danych: Obecne dane są zagregowane. Dostęp do danych surowych (każdy wiersz = jedna interakcja) pozwoliłby na głębszą analizę, np. dystrybucji czasów rozmów, a nie tylko średniej.
Przystępuję do analizy, zakładając, że posiadamy kolumnę liczba_polaczen.
🐍 3. Przygotowanie Środowiska i Wczytanie Danych
Zaczynamy od importu niezbędnych bibliotek i zdefiniowania połączenia z bazą danych MS SQL Server.
# Import bibliotek
import pandas as pd
import sqlalchemy as sa
import matplotlib.pyplot as plt
import seaborn as sns

# Ustawienia wizualizacji
sns.set_theme(style="whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['axes.titlesize'] = 16
plt.rcParams['axes.labelsize'] = 12

# --- Ładowanie danych z MS SQL Server ---
# Wypełnij poniższe zmienne swoimi danymi dostępowymi
server_name = "TWOJ_SERWER"
database_name = "TWOJA_BAZA_DANYCH"
table_name = "NAZWA_TWOJEJ_TABELI"
# Użyj uwierzytelniania Windows (Integrated Security) lub SQL Server Auth
# Dla Windows Auth:
connection_string = f"mssql+pyodbc://{server_name}/{database_name}?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes"

# Stworzenie "silnika" połączenia
try:
    engine = sa.create_engine(connection_string)
    
    # Zapytanie SQL do wczytania danych
    query = f"SELECT * FROM {table_name}"
    
    # Wczytanie danych do ramki danych Pandas
    df = pd.read_sql(query, engine)
    
    print("✅ Pomyślnie połączono i wczytano dane.")
    print("Próbka danych:")
    display(df.head())

except Exception as e:
    print(f"❌ Błąd podczas połączenia z bazą danych: {e}")
    # W przypadku błędu, tworzę przykładową ramkę danych, aby kontynuować analizę
    data = {'Data': ['2023-01-05', '2023-01-05', '2024-01-06', '2024-01-06', '2023-02-10', '2024-02-11'],
            'Kanal_kontakt': ['voice', 'czat', 'voice', 'czat', 'voice', 'wideo'],
            'Typ_interakcji': ['telefon', 'aplikacja mobilna', 'telefon', 'aplikacja mobilna', 'telefon', 'aplikacja desktopowa'],
            'czy_odebrane': [1, 1, 1, 0, 1, 1],
            'wejscie': ['ivr', 'bot', 'ivr', 'bot', 'ivr', 'vendor'],
            'gdzie_koniec': ['konsultant', 'konsultant', 'konsultant', 'bot', 'transfer', 'konsultant'],
            'czas_rozmowy': [18000, 5400, 22000, 0, 15000, 7200],
            'liczba_polaczen': [100, 60, 110, 50, 90, 30]}
    df = pd.DataFrame(data)
    print("⚠️ Utworzono przykładowy zbiór danych do dalszej analizy.")
    display(df.head())


🛠️ 4. Przygotowanie Danych (Data Preprocessing & Feature Engineering)
Dane w stanie surowym rzadko nadają się do bezpośredniej analizy. Musimy je oczyścić i wzbogacić o dodatkowe, użyteczne cechy.
# Konwersja kolumny 'Data' na typ daty
df['Data'] = pd.to_datetime(df['Data'])

# Inżynieria cech - wyciągnięcie roku, miesiąca i dnia tygodnia
df['Rok'] = df['Data'].dt.year
df['Miesiac'] = df['Data'].dt.month
df['Nazwa_Miesiaca'] = df['Data'].dt.strftime('%B')
df['Dzien_Tygodnia'] = df['Data'].dt.day_name()

# Obliczenie kluczowych metryk
# AHT - Average Handle Time (Średni Czas Obsługi) w sekundach
# Używamy .div() i .fillna(0) aby uniknąć dzielenia przez zero, jeśli dla jakiejś grupy odebrane połączenia mają sumaryczny czas 0
df['AHT_s'] = (df['czas_rozmowy'] / df['liczba_polaczen'].where(df['czy_odebrane'] == 1, 0)).fillna(0)

# Określenie lat do analizy
current_year = df['Rok'].max()
previous_year = current_year - 1

df_cy = df[df['Rok'] == current_year]
df_py = df[df['Rok'] == previous_year]

print(f"Analiza porównawcza dla lat: {current_year} (CY) vs {previous_year} (PY)")
print("Nowe kolumny i AHT zostały obliczone.")

📊 5. Analiza Porównawcza i Wizualizacja Danych
Teraz przechodzimy do serca analizy – porównania wyników rok do roku.
5.1. Ogólny Wolumen Interakcji
Sprawdźmy, jak zmieniła się całkowita liczba obsługiwanych interakcji.
# Agregacja roczna
total_volume = df.groupby('Rok')['liczba_polaczen'].sum().reset_index()
total_volume_cy = total_volume[total_volume['Rok'] == current_year]['liczba_polaczen'].sum()
total_volume_py = total_volume[total_volume['Rok'] == previous_year]['liczba_polaczen'].sum()

# Obliczenie zmiany YoY
yoy_change = ((total_volume_cy - total_volume_py) / total_volume_py) * 100

# Wizualizacja
plt.figure(figsize=(8, 5))
ax = sns.barplot(data=total_volume, x='Rok', y='liczba_polaczen', palette='viridis')
ax.set_title(f'Całkowity Wolumen Interakcji YoY\nZmiana: {yoy_change:.2f}%')
for p in ax.patches:
    ax.annotate(f'{int(p.get_height()):,}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', fontsize=12, color='black', xytext=(0, 5),
                textcoords='offset points')
plt.ylabel('Liczba Połączeń')
plt.xlabel('Rok')
plt.show()

Wnioski:
 * Na podstawie wykresu widać [trend wzrostowy/spadkowy].
 * Całkowity wolumen interakcji zmienił się o yoy_change%. Taka zmiana może wynikać ze wzrostu bazy klientów, problemów z produktem lub skuteczności kampanii marketingowych. Należy to skorelować z działaniami biznesowymi firmy w analizowanym okresie.
5.2. Analiza Wolumenu w Podziale na Kanały
Które kanały napędzają zmianę w wolumenie?
# Agregacja po roku i kanale
channel_volume = df.groupby(['Rok', 'Kanal_kontakt'])['liczba_polaczen'].sum().reset_index()

# Wizualizacja
plt.figure(figsize=(14, 7))
sns.barplot(data=channel_volume, x='Kanal_kontakt', y='liczba_polaczen', hue='Rok', palette='mako')
plt.title('Wolumen Interakcji w Podziale na Kanały (YoY)')
plt.ylabel('Liczba Połączeń')
plt.xlabel('Kanał Kontaktu')
plt.legend(title='Rok')
plt.show()

Wnioski:
 * Ten wykres pokazuje, które kanały zyskują na popularności. Widzimy wyraźny wzrost/spadek dla kanału X, podczas gdy kanał Y pozostaje stabilny/traci.
 * Rekomendacja: Jeśli obserwujemy silny wzrost na kanale czat, warto zainwestować w rozwój bota lub zwiększyć liczbę agentów obsługujących ten kanał. Jeśli kanał voice maleje, można rozważyć optymalizację zasobów w tym obszarze.
5.3. Analiza Średniego Czasu Obsługi (AHT)
Czy nasza efektywność rośnie czy maleje? AHT jest kluczowym wskaźnikiem.
# Agregacja AHT dla połączeń odebranych
aht_analysis = df[df['czy_odebrane'] == 1].copy()
# Obliczamy średnią ważoną AHT
aht_yearly = aht_analysis.groupby('Rok').apply(lambda x: (x['czas_rozmowy']).sum() / x['liczba_polaczen'].sum()).reset_index(name='Sredni_AHT_s')


# Wizualizacja AHT
plt.figure(figsize=(8, 5))
ax = sns.barplot(data=aht_yearly, x='Rok', y='Sredni_AHT_s', palette='coolwarm')
ax.set_title('Średni Czas Obsługi (AHT) YoY')
for p in ax.patches:
    ax.annotate(f'{p.get_height():.1f}s', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', fontsize=12, color='black', xytext=(0, 5),
                textcoords='offset points')
plt.ylabel('AHT (w sekundach)')
plt.xlabel('Rok')
plt.show()

Wnioski:
 * Wzrost AHT może świadczyć o bardziej skomplikowanych problemach zgłaszanych przez klientów, niedostatecznym szkoleniu agentów lub problemach systemowych.
 * Spadek AHT to generalnie dobry znak, o ile nie odbywa się kosztem jakości (dlatego potrzebne są dane CSAT!). Może być wynikiem lepszych narzędzi dla konsultantów lub skuteczniejszego sortowania zapytań przez IVR/bota.
5.4. Analiza Ścieżki Klienta: Wejście i Zakończenie Interakcji
Gdzie klienci zaczynają i kończą swoją podróż? Czy bot/IVR skutecznie filtruje połączenia?
fig, axes = plt.subplots(1, 2, figsize=(18, 7))

# Analiza punktów wejścia
entry_points = df.groupby(['Rok', 'wejscie'])['liczba_polaczen'].sum().reset_index()
sns.barplot(ax=axes[0], data=entry_points, x='wejscie', y='liczba_polaczen', hue='Rok', palette='viridis')
axes[0].set_title('Punkty Wejścia Interakcji (YoY)')
axes[0].set_xlabel('Punkt Wejścia')
axes[0].set_ylabel('Liczba Połączeń')

# Analiza punktów zakończenia
exit_points = df.groupby(['Rok', 'gdzie_koniec'])['liczba_polaczen'].sum().reset_index()
sns.barplot(ax=axes[1], data=exit_points, x='gdzie_koniec', y='liczba_polaczen', hue='Rok', palette='plasma')
axes[1].set_title('Punkty Zakończenia Interakcji (YoY)')
axes[1].set_xlabel('Punkt Zakończenia')
axes[1].set_ylabel('Liczba Połączeń')

plt.tight_layout()
plt.show()

Wnioski:
 * Po lewej (Wejścia): Analizujemy, czy np. więcej klientów korzysta z bota jako pierwszego punktu kontaktu.
 * Po prawej (Zakończenia): Kluczowe jest tu porównanie liczby interakcji kończących się na bocie/IVR w stosunku do tych trafiających do konsultanta. Jeśli rok do roku rośnie liczba spraw załatwianych w pełni automatycznie, to jest to duży sukces i oszczędność kosztów. Jeśli rośnie liczba transferów (transfer), może to wskazywać na złą konfigurację IVR lub bota.
5.5. Analiza Sezonowości i Trendów Miesięcznych
Jak rozkłada się ruch w ciągu roku? Czy występują podobne piki w obu latach?
# Agregacja miesięczna
monthly_trends = df.groupby(['Rok', 'Miesiac'])['liczba_polaczen'].sum().reset_index()

# Wizualizacja
plt.figure(figsize=(14, 7))
sns.lineplot(data=monthly_trends, x='Miesiac', y='liczba_polaczen', hue='Rok', palette='Set1', marker='o', linewidth=2.5)
plt.title('Miesięczne Trendy Wolumenu Połączeń (YoY)')
plt.xlabel('Miesiąc')
plt.ylabel('Liczba Połączeń')
plt.xticks(ticks=range(1, 13), labels=['Sty', 'Lut', 'Mar', 'Kwi', 'Maj', 'Cze', 'Lip', 'Sie', 'Wrz', 'Paź', 'Lis', 'Gru'])
plt.legend(title='Rok')
plt.show()

Wnioski:
 * Wykres liniowy doskonale uwidacznia sezonowość. Możemy zidentyfikować miesiące o największym obciążeniu (np. okresy świąteczne, początek/koniec roku, daty związane z cyklem życia produktu).
 * Porównanie linii dla obu lat pozwala sprawdzić, czy piki sezonowe się powtarzają i czy różnica w wolumenie między latami jest stała, czy może rośnie w określonych miesiącach.
 * Rekomendacja: Wiedza o sezonowości jest kluczowa dla planowania grafików pracy konsultantów (Workforce Management).
💡 6. Podsumowanie i Rekomendacje Biznesowe
Na podstawie przeprowadzonej analizy wyłaniają się następujące kluczowe wnioski:
 * Ogólny Trend: W roku {current_year} zaobserwowaliśmy [wzrost/spadek] całkowitego wolumenu interakcji o yoy_change% w stosunku do roku {previous_year}.
 * Główny Motor Zmian: Największy wpływ na tę zmianę miał kanał [nazwa kanału], którego popularność [wzrosła/zmalała]. Wskazuje to na zmieniające się preferencje klientów w sposobie kontaktu z nami.
 * Efektywność Operacyjna: Średni czas obsługi (AHT) [wzrósł/zmniejszył się], co sugeruje, że interakcje stają się [bardziej/mniej] skomplikowane lub nasza efektywność uległa zmianie. Należy to zbadać w kontekście konkretnych typów zgłoszeń.
 * Automatyzacja i Self-Service: Analiza ścieżki klienta pokazała, że odsetek interakcji kończących się w kanałach automatycznych (bot, IVR) [wzrósł/zmniejszył się]. To kluczowy wskaźnik sukcesu naszej strategii automatyzacji.
Rekomendacje:
 * Inwestycja w kanały rosnące: Zwiększyć zasoby (ludzkie i technologiczne) dla kanału [nazwa rosnącego kanału], aby utrzymać wysoką jakość obsługi przy rosnącym wolumenie.
 * Dogłębna analiza AHT: Przeprowadzić analizę "drill-down" dla interakcji z rosnącym AHT. Zidentyfikować konkretne przyczyny (np. typy zgłoszeń) i zorganizować dodatkowe szkolenia dla agentów lub usprawnić bazę wiedzy.
 * Optymalizacja ścieżki klienta: Jeśli zbyt wiele interakcji z bota/IVR trafia do konsultanta, należy przeanalizować logi i zidentyfikować miejsca, w których proces się "zrywa". Celem jest zwiększenie wskaźnika FCR w kanałach automatycznych.
 * Prognozowanie i planowanie: Wykorzystać zidentyfikowane trendy miesięczne do stworzenia bardziej dokładnego modelu prognozowania wolumenu, co pozwoli na lepszą optymalizację grafików pracy.

Doskonale, to bardzo ważna i częsta sytuacja w analizie danych! Praca na danych surowych (per połączenie) daje nam znacznie większe możliwości. Porównanie niepełnego roku do adekwatnego okresu w przeszłości to standard "best practice".
Przygotowałem zaktualizowaną wersję notatnika, która uwzględnia nową strukturę danych i logikę porównawczą "rok do dziś" (Year-to-Date, YTD).
Analiza Porównawcza Połączeń z Infolinią - Wersja YTD (Year-to-Date)
Data: 16 września 2025
Autor: Gemini - Analityk Danych
🎯 1. Cel Analizy (Bez Zmian)
Głównym celem tej analizy jest przeprowadzenie szczegółowego porównania rocznego (Year-over-Year) kluczowych wskaźników wydajności (KPI) infolinii dla porównywalnych okresów. Skupimy się na zrozumieniu zmian w wolumenie, efektywności i charakterystyce interakcji klientów.
🤔 2. Założenia i Dane
Kluczowa zmiana: Pracujemy na danych granularnych, gdzie każdy wiersz to jedna, unikalna interakcja identyfikowana przez callid. Tabela nie jest już pogrupowana.
Struktura danych:
 * callid: Unikalny identyfikator każdej interakcji.
 * Data: Data interakcji (zakładam, że zawiera również czas, jeśli nie, analiza godzinowa będzie niemożliwa).
 * Pozostałe kolumny bez zmian: Kanal_kontakt, Typ_interakcji, czy_odebrane, wejscie, gdzie_koniec, czas_rozmowy.
Logika porównawcza:
Analiza będzie porównywać okres od początku roku do ostatniej dostępnej daty w bieżącym roku z dokładnie tym samym okresem w roku poprzednim. Na przykład, jeśli najnowsze dane są z 15 września 2024, porównamy okres 01.01.2024 - 15.09.2024 z 01.01.2023 - 15.09.2023.
🐍 3. Przygotowanie Środowiska i Wczytanie Danych
Kod do połączenia pozostaje ten sam, ale tworzę nowy, bardziej realistyczny przykład danych.
# Import bibliotek
import pandas as pd
import sqlalchemy as sa
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np # Dodajemy numpy do obliczeń

# Ustawienia wizualizacji
sns.set_theme(style="whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

# --- Ładowanie danych z MS SQL Server ---
# Wypełnij poniższe zmienne swoimi danymi dostępowymi
server_name = "TWOJ_SERWER"
database_name = "TWOJA_BAZA_DANYCH"
table_name = "NAZWA_TWOJEJ_TABELI_RAW"

try:
    # ... (kod połączenia taki sam jak poprzednio) ...
    query = f"SELECT * FROM {table_name}"
    df_raw = pd.read_sql(query, engine)
    print("✅ Pomyślnie wczytano dane granularne.")

except Exception as e:
    print(f"❌ Błąd: {e}")
    # Nowy, przykładowy zbiór danych granularnych
    data = {'callid': range(1000),
            'Data': pd.to_datetime(pd.date_range(start='2023-01-01', periods=500, freq='D').tolist() + 
                                   pd.date_range(start='2024-01-01', periods=500, freq='D').tolist()),
            'Kanal_kontakt': np.random.choice(['voice', 'czat', 'wideo'], 1000),
            'czy_odebrane': np.random.choice([1, 0], 1000, p=[0.8, 0.2]),
            'wejscie': np.random.choice(['ivr', 'bot', 'vendor'], 1000),
            'gdzie_koniec': np.random.choice(['konsultant', 'bot', 'transfer'], 1000),
            'czas_rozmowy': np.random.randint(30, 600, 1000)}
    df_raw = pd.DataFrame(data)
    # Ustawiamy niektóre czasy na 0 dla nieodebranych
    df_raw.loc[df_raw['czy_odebrane'] == 0, 'czas_rozmowy'] = 0
    print("⚠️ Utworzono przykładowy, granularny zbiór danych.")
    
display(df_raw.head())

🛠️ 4. Przygotowanie Danych (Logika YTD)
To jest kluczowy, nowy krok. Tworzymy porównywalne zbiory danych.
# Konwersja kolumny 'Data' i tworzenie cech
df_raw['Data'] = pd.to_datetime(df_raw['Data'])
df_raw['Rok'] = df_raw['Data'].dt.year
df_raw['Miesiac'] = df_raw['Data'].dt.month
df_raw['Dzien_Roku'] = df_raw['Data'].dt.dayofyear # Dzień roku (1-366) jest idealny do porównań YTD

# --- Implementacja logiki Year-to-Date (YTD) ---

# 1. Znajdź ostatnią datę w bieżącym roku
current_year = df_raw['Rok'].max()
latest_date_cy = df_raw[df_raw['Rok'] == current_year]['Data'].max()
latest_day_of_year = latest_date_cy.dayofyear

print(f"Najnowsze dane pochodzą z: {latest_date_cy.strftime('%Y-%m-%d')}")
print(f"Analiza zostanie przeprowadzona do {latest_day_of_year}. dnia roku.")

# 2. Odfiltruj dane z obu lat, aby obejmowały ten sam okres
df_ytd = df_raw[df_raw['Dzien_Roku'] <= latest_day_of_year].copy()

# 3. Sprawdzenie
print("\nZakres dat w analizowanym zbiorze:")
print(f"Bieżący rok: {df_ytd[df_ytd['Rok'] == current_year]['Data'].min().date()} - {df_ytd[df_ytd['Rok'] == current_year]['Data'].max().date()}")
print(f"Poprzedni rok: {df_ytd[df_ytd['Rok'] == current_year-1]['Data'].min().date()} - {df_ytd[df_ytd['Rok'] == current_year-1]['Data'].max().date()}")


📊 5. Analiza Porównawcza i Wizualizacja Danych (YTD)
Wszystkie poniższe analizy używają już przefiltrowanego zbioru df_ytd.
5.1. Ogólny Wolumen Interakcji (YTD)
# Agregacja YTD - liczymy callid
total_volume_ytd = df_ytd.groupby('Rok')['callid'].count().reset_index().rename(columns={'callid': 'liczba_polaczen'})

# Obliczenia i wizualizacja (kod wizualizacji pozostaje bardzo podobny)
# ... (użyj kodu z poprzedniej odpowiedzi, ale na ramce total_volume_ytd) ...
# Pamiętaj o zmianie tytułu na 'Całkowity Wolumen Interakcji (YTD)'

Wnioski (YTD):
 * Porównując adekwatne okresy, widzimy [wzrost/spadek] wolumenu. Dzięki temu podejściu eliminujemy wpływ sezonowości z pozostałej części roku, dając bardziej rzetelny obraz sytuacji.
5.2. Analiza Wolumenu w Podziale na Kanały (YTD)
# Agregacja YTD po kanałach
channel_volume_ytd = df_ytd.groupby(['Rok', 'Kanal_kontakt'])['callid'].count().reset_index().rename(columns={'callid': 'liczba_polaczen'})

# Wizualizacja (kod podobny, zmień tytuł na '... (YTD)')
# ...

Wnioski (YTD):
 * Analiza YTD potwierdza/zmienia obraz z analizy całorocznej. Być może wzrost na kanale czat nastąpił dopiero w drugiej połowie poprzedniego roku, a porównanie YTD pokazuje bardziej wyrównany poziom.
5.3. Analiza Średniego Czasu Obsługi (AHT) (YTD)
Teraz AHT liczymy jako prostą średnią z czas_rozmowy dla połączeń odebranych.
# Filtrujemy tylko odebrane połączenia
df_answered_ytd = df_ytd[df_ytd['czy_odebrane'] == 1]

# Agregacja YTD AHT
aht_ytd = df_answered_ytd.groupby('Rok')['czas_rozmowy'].mean().reset_index().rename(columns={'czas_rozmowy': 'Sredni_AHT_s'})

# Wizualizacja (kod podobny, zmień tytuł na '... (YTD)')
# ...

Wnioski (YTD):
 * Analiza AHT w okresach porównywalnych jest kluczowa. Być może w poprzednim roku pod koniec roku mieliśmy serię trudnych zgłoszeń, które zawyżyły średnią roczną, a analiza YTD pokazuje, że nasza efektywność faktycznie się poprawiła.
(Pozostałe analizy 5.4 i 5.5 wykonuje się analogicznie, zawsze grupując dane z df_ytd i licząc callid za pomocą .count())
💎 6. Głębsza Analiza Dzięki Danym Granularnym (Bonus)
Praca na surowych danych otwiera drzwi do analiz, które wcześniej były niemożliwe.
6.1. Dystrybucja Czasu Trwania Rozmów
Średnia (AHT) to nie wszystko. Czy większość rozmów jest krótka, a kilka ekstremalnie długich zawyża średnią?
plt.figure(figsize=(14, 7))
# Używamy danych z bieżącego roku jako przykład
sns.histplot(data=df_answered_ytd[df_answered_ytd['Rok'] == current_year], x='czas_rozmowy', bins=50, kde=True)
plt.title(f'Dystrybucja Czasu Trwania Rozmów (Histogram) w {current_year} (YTD)')
plt.xlabel('Czas rozmowy (sekundy)')
plt.ylabel('Liczba połączeń')
plt.axvline(df_answered_ytd[df_answered_ytd['Rok'] == current_year]['czas_rozmowy'].mean(), color='red', linestyle='--', label='Średnia (AHT)')
plt.legend()
plt.show()

Wnioski:
 * Histogram pokazuje faktyczny rozkład czasów rozmów. Jeśli jest on prawostronnie skośny (co jest typowe), oznacza to, że większość rozmów jest krótsza niż średnia, ale mamy "ogon" bardzo długich rozmów.
 * Rekomendacja: Zamiast skupiać się tylko na obniżaniu średniej, warto zidentyfikować te najdłuższe rozmowy (callid z top 5% czas_rozmowy) i zbadać ich przyczyny. Być może są to złożone problemy techniczne lub nowi agenci, którzy potrzebują wsparcia.
6.2. Analiza obciążenia w ciągu dnia (Peak Hour Analysis)
Jeśli kolumna Data zawiera czas, możemy sprawdzić, o której godzinie mamy najwięcej połączeń.
# Wzbogacenie danych o godzinę
df_ytd['Godzina'] = df_ytd['Data'].dt.hour

# Agregacja godzinowa dla bieżącego roku
hourly_volume = df_ytd[df_ytd['Rok'] == current_year].groupby('Godzina')['callid'].count().reset_index()

plt.figure(figsize=(14, 7))
sns.barplot(data=hourly_volume, x='Godzina', y='callid', color='skyblue')
plt.title(f'Rozkład Połączeń w Ciągu Dnia w {current_year} (YTD)')
plt.xlabel('Godzina Dnia')
plt.ylabel('Liczba Połączeń')
plt.show()

Wnioski:
 * Ten wykres jest absolutnie kluczowy dla Workforce Managementu. Pozwala zidentyfikować godziny szczytu (np. 10:00-12:00) oraz okresy mniejszego ruchu.
 * Rekomendacja: Dopasować grafiki konsultantów do faktycznego obciążenia. W godzinach szczytu zapewnić pełną obsadę, a w godzinach o mniejszym ruchu zaplanować czas na szkolenia, spotkania czy odpisywanie na maile.
💡 7. Podsumowanie i Rekomendacje (Wersja YTD)
Ta zaktualizowana analiza, oparta na danych granularnych i porównaniu YTD, daje znacznie dokładniejszy i bardziej wiarygodny obraz sytuacji.
Kluczowe zmiany w stosunku do poprzedniej analizy:
 * Wiarygodność: Porównujemy te same, adekwatne okresy, co eliminuje błędy wynikające z sezonowości w późniejszych miesiącach.
 * Głębia: Analiza dystrybucji AHT i obciążenia godzinowego dostarcza nowych, praktycznych wniosków, które były niemożliwe do uzyskania na danych zagregowanych.
Rekomendacje z poprzedniej analizy pozostają w mocy, ale teraz są poparte solidniejszymi danymi i wzbogacone o nowe, bardziej szczegółowe sugestie dotyczące zarządzania najdłuższymi rozmowami i planowania grafików pracy.

Jasne, oto kompletny kod analizy w jednym pliku, gotowy do wklejenia i uruchomienia w notatniku Jupyter. Plik zawiera wszystkie kroki: od połączenia z danymi, przez ich przygotowanie z logiką YTD, aż po analizę, wizualizację i wnioski biznesowe.
# ==============================================================================
# KROK 1: IMPORT BIBLIOTEK I USTAWIENIA WIZUALIZACJI
# ==============================================================================
import pandas as pd
import sqlalchemy as sa
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Ustawienia dla wykresów, aby były czytelne i estetyczne
sns.set_theme(style="whitegrid")
plt.rcParams['figure.figsize'] = (14, 7)
plt.rcParams['axes.titlesize'] = 18
plt.rcParams['axes.labelsize'] = 14
plt.rcParams['xtick.labelsize'] = 12
plt.rcParams['ytick.labelsize'] = 12
print("✅ Biblioteki załadowane, styl wykresów ustawiony.")


# ==============================================================================
# KROK 2: WCZYTANIE DANYCH Z BAZY MS SQL SERVER
# ==============================================================================
# Wypełnij poniższe zmienne swoimi danymi dostępowymi
server_name = "TWOJ_SERWER"
database_name = "TWOJA_BAZA_DANYCH"
table_name = "NAZWA_TWOJEJ_TABELI_RAW"  # Tabela z danymi granularnymi (niezagregowanymi)

try:
    # Tworzenie connection stringa dla uwierzytelniania Windows
    connection_string = f"mssql+pyodbc://{server_name}/{database_name}?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes"
    engine = sa.create_engine(connection_string)
    
    query = f"SELECT * FROM {table_name}"
    df_raw = pd.read_sql(query, engine)
    
    print(f"✅ Pomyślnie połączono i wczytano {len(df_raw)} rekordów z tabeli {table_name}.")

except Exception as e:
    print(f"❌ Błąd podczas połączenia z bazą danych: {e}")
    print("⚠️ Tworzę przykładowy, granularny zbiór danych, aby kontynuować analizę.")
    # Generowanie realistycznych danych przykładowych
    date_rng_2024 = pd.to_datetime(pd.date_range(start='2024-01-01', end='2024-09-15', freq='10min'))
    date_rng_2023 = pd.to_datetime(pd.date_range(start='2023-01-01', end='2023-12-31', freq='9min'))
    all_dates = date_rng_2024.union(date_rng_2023)
    
    data = {
        'callid': range(len(all_dates)),
        'Data': all_dates,
        'Kanal_kontakt': np.random.choice(['voice', 'czat', 'wideo'], len(all_dates), p=[0.7, 0.25, 0.05]),
        'czy_odebrane': np.random.choice([1, 0], len(all_dates), p=[0.85, 0.15]),
        'wejscie': np.random.choice(['ivr', 'bot', 'vendor'], len(all_dates), p=[0.5, 0.4, 0.1]),
        'gdzie_koniec': np.random.choice(['konsultant', 'bot', 'transfer', 'ivr'], len(all_dates), p=[0.7, 0.15, 0.1, 0.05]),
        'czas_rozmowy': np.random.gamma(2, 150, len(all_dates)).astype(int)
    }
    df_raw = pd.DataFrame(data)
    df_raw.loc[df_raw['czy_odebrane'] == 0, 'czas_rozmowy'] = 0

print("\n--- Próbka danych (5 pierwszych wierszy) ---")
display(df_raw.head())


# ==============================================================================
# KROK 3: PRZYGOTOWANIE DANYCH I IMPLEMENTACJA LOGIKI YTD
# ==============================================================================
print("\n--- Rozpoczęto przygotowanie danych ---")
# Konwersja kolumny 'Data' na typ daty i tworzenie dodatkowych cech czasowych
df_raw['Data'] = pd.to_datetime(df_raw['Data'])
df_raw['Rok'] = df_raw['Data'].dt.year
df_raw['Miesiac'] = df_raw['Data'].dt.month
df_raw['Dzien_Roku'] = df_raw['Data'].dt.dayofyear # Klucz do porównań YTD
df_raw['Godzina'] = df_raw['Data'].dt.hour
df_raw['Nazwa_Miesiaca'] = df_raw['Data'].dt.strftime('%B')

# --- Implementacja logiki Year-to-Date (YTD) ---
# 1. Znajdź ostatnią datę i dzień roku w bieżącym roku
current_year = df_raw['Rok'].max()
previous_year = current_year - 1
latest_date_cy = df_raw[df_raw['Rok'] == current_year]['Data'].max()
latest_day_of_year = latest_date_cy.dayofyear

print(f"Bieżący rok w analizie: {current_year}")
print(f"Poprzedni rok w analizie: {previous_year}")
print(f"Najnowsze dane pochodzą z: {latest_date_cy.strftime('%Y-%m-%d')} (to {latest_day_of_year}. dzień roku)")

# 2. Odfiltruj dane, aby obejmowały ten sam okres w obu latach
df_ytd = df_raw[df_raw['Dzien_Roku'] <= latest_day_of_year].copy()
print(f"✅ Dane odfiltrowane. Analiza zostanie przeprowadzona dla okresu od 1 stycznia do {latest_day_of_year}. dnia roku.")

# ==============================================================================
# KROK 4: ANALIZA PORÓWNAWCZA I WIZUALIZACJA DANYCH (YTD)
# ==============================================================================
print("\n--- Rozpoczęto główną analizę porównawczą (YTD) ---")

# --- 4.1. Ogólny Wolumen Interakcji (YTD) ---
total_volume_ytd = df_ytd.groupby('Rok')['callid'].count().reset_index().rename(columns={'callid': 'liczba_polaczen'})
vol_cy = total_volume_ytd.loc[total_volume_ytd['Rok'] == current_year, 'liczba_polaczen'].iloc[0]
vol_py = total_volume_ytd.loc[total_volume_ytd['Rok'] == previous_year, 'liczba_polaczen'].iloc[0]
yoy_change = ((vol_cy - vol_py) / vol_py) * 100

plt.figure(figsize=(10, 6))
ax = sns.barplot(data=total_volume_ytd, x='Rok', y='liczba_polaczen', palette='viridis')
ax.set_title(f'Całkowity Wolumen Interakcji (YTD)\nZmiana: {yoy_change:.2f}%', fontsize=20)
for p in ax.patches:
    ax.annotate(f'{int(p.get_height()):,}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', fontsize=14, color='black', xytext=(0, 10),
                textcoords='offset points')
plt.ylabel('Liczba Połączeń')
plt.xlabel('Rok')
plt.show()

# --- 4.2. Analiza Wolumenu w Podziale na Kanały (YTD) ---
channel_volume_ytd = df_ytd.groupby(['Rok', 'Kanal_kontakt'])['callid'].count().reset_index().rename(columns={'callid': 'liczba_polaczen'})
sns.barplot(data=channel_volume_ytd, x='Kanal_kontakt', y='liczba_polaczen', hue='Rok', palette='mako')
plt.title('Wolumen Interakcji w Podziale na Kanały (YTD)')
plt.ylabel('Liczba Połączeń')
plt.xlabel('Kanał Kontaktu')
plt.legend(title='Rok')
plt.show()

# --- 4.3. Analiza Średniego Czasu Obsługi (AHT) (YTD) ---
df_answered_ytd = df_ytd[df_ytd['czy_odebrane'] == 1]
aht_ytd = df_answered_ytd.groupby('Rok')['czas_rozmowy'].mean().reset_index().rename(columns={'czas_rozmowy': 'Sredni_AHT_s'})
plt.figure(figsize=(10, 6))
ax = sns.barplot(data=aht_ytd, x='Rok', y='Sredni_AHT_s', palette='coolwarm')
ax.set_title('Średni Czas Obsługi - AHT (YTD)')
for p in ax.patches:
    ax.annotate(f'{p.get_height():.1f}s', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', fontsize=14, color='black', xytext=(0, 10),
                textcoords='offset points')
plt.ylabel('AHT (w sekundach)')
plt.xlabel('Rok')
plt.show()

# --- 4.4. Analiza Ścieżki Klienta: Wejście i Zakończenie (YTD) ---
fig, axes = plt.subplots(1, 2, figsize=(20, 8))
entry_points_ytd = df_ytd.groupby(['Rok', 'wejscie'])['callid'].count().reset_index()
exit_points_ytd = df_ytd.groupby(['Rok', 'gdzie_koniec'])['callid'].count().reset_index()

sns.barplot(ax=axes[0], data=entry_points_ytd, x='wejscie', y='callid', hue='Rok', palette='viridis')
axes[0].set_title('Punkty Wejścia Interakcji (YTD)')
axes[0].set_ylabel('Liczba Połączeń')

sns.barplot(ax=axes[1], data=exit_points_ytd, x='gdzie_koniec', y='callid', hue='Rok', palette='plasma')
axes[1].set_title('Punkty Zakończenia Interakcji (YTD)')
axes[1].set_ylabel('Liczba Połączeń')
plt.tight_layout()
plt.show()

# --- 4.5. Analiza Sezonowości i Trendów Miesięcznych (YTD) ---
monthly_trends_ytd = df_ytd.groupby(['Rok', 'Miesiac'])['callid'].count().reset_index()
sns.lineplot(data=monthly_trends_ytd, x='Miesiac', y='callid', hue='Rok', palette='Set1', marker='o', linewidth=2.5)
plt.title('Miesięczne Trendy Wolumenu Połączeń (YTD)')
plt.xlabel('Miesiąc')
plt.ylabel('Liczba Połączeń')
plt.xticks(ticks=range(1, 13))
plt.legend(title='Rok')
plt.grid(True, which='both', linestyle='--')
plt.show()

# ==============================================================================
# KROK 5: GŁĘBSZA ANALIZA DZIĘKI DANYM GRANULARNYM (BONUS)
# ==============================================================================
print("\n--- Rozpoczęto analizy dodatkowe (bonus) ---")

# --- 5.1. Dystrybucja Czasu Trwania Rozmów ---
fig, axes = plt.subplots(1, 2, figsize=(20, 8), sharey=True)
sns.histplot(ax=axes[0], data=df_answered_ytd[df_answered_ytd['Rok'] == previous_year], x='czas_rozmowy', bins=50, kde=True)
axes[0].set_title(f'Dystrybucja Czasu Rozmów w {previous_year} (YTD)')
axes[0].axvline(aht_ytd.loc[aht_ytd['Rok'] == previous_year, 'Sredni_AHT_s'].iloc[0], color='red', linestyle='--', label='Średnia (AHT)')
axes[0].legend()

sns.histplot(ax=axes[1], data=df_answered_ytd[df_answered_ytd['Rok'] == current_year], x='czas_rozmowy', bins=50, kde=True, color='orange')
axes[1].set_title(f'Dystrybucja Czasu Rozmów w {current_year} (YTD)')
axes[1].axvline(aht_ytd.loc[aht_ytd['Rok'] == current_year, 'Sredni_AHT_s'].iloc[0], color='red', linestyle='--', label='Średnia (AHT)')
axes[1].legend()
plt.suptitle('Porównanie Dystrybucji Czasu Rozmów (YTD)', fontsize=22)
plt.show()

# --- 5.2. Analiza obciążenia w ciągu dnia (Peak Hour Analysis) ---
hourly_volume_ytd = df_ytd.groupby(['Rok', 'Godzina'])['callid'].count().reset_index()
g = sns.FacetGrid(hourly_volume_ytd, col="Rok", height=6, aspect=1.5)
g.map(sns.barplot, "Godzina", "callid", order=range(24), palette=['skyblue', 'lightcoral'])
g.set_axis_labels("Godzina Dnia", "Liczba Połączeń")
g.set_titles(col_template="Rok {col_name}")
g.fig.suptitle('Rozkład Połączeń w Ciągu Dnia (YTD)', y=1.03, fontsize=20)
plt.show()

# ==============================================================================
# KROK 6: PODSUMOWANIE I REKOMENDACJE BIZNESOWE
# ==============================================================================
print("\n\n" + "="*80)
print("💡 PODSUMOWANIE WNIOSKÓW I REKOMENDACJE BIZNESOWE 💡")
print("="*80)
print(f"""
Analiza porównawcza okresu od 1 stycznia do {latest_date_cy.strftime('%d %B')} dla lat {previous_year} i {current_year} wykazała następujące trendy:

1.  **Ogólny Wolumen:** Zaobserwowaliśmy {'wzrost' if yoy_change > 0 else 'spadek'} całkowitego wolumenu interakcji o {yoy_change:.2f}%. 
    Jest to kluczowa informacja dla planowania budżetu i zasobów na kolejne okresy.

2.  **Struktura Kanałów:** Analiza pokazała, że głównym motorem zmian był kanał [tutaj wpisz nazwę kanału z największą zmianą].
    Rekomendacja: Należy dostosować strategię obsługi do zmieniających się preferencji klientów, potencjalnie inwestując więcej w rozwój kanałów cyfrowych jak 'czat' lub optymalizując obsługę 'voice'.

3.  **Efektywność Operacyjna (AHT):** Średni czas obsługi (AHT) zmienił się z {aht_ytd.loc[aht_ytd['Rok'] == previous_year, 'Sredni_AHT_s'].iloc[0]:.1f}s do {aht_ytd.loc[aht_ytd['Rok'] == current_year, 'Sredni_AHT_s'].iloc[0]:.1f}s.
    Rekomendacja: Warto zbadać przyczyny tej zmiany. Jeśli AHT rośnie, należy zidentyfikować najbardziej czasochłonne typy zgłoszeń i zorganizować szkolenia. Jeśli maleje, upewnić się, że nie cierpi na tym jakość (CSAT).

4.  **Automatyzacja (Ścieżka Klienta):** Porównanie punktów wejścia i zakończenia interakcji wskazuje na [wzrost/spadek] skuteczności naszych systemów automatycznych (IVR/Bot).
    Rekomendacja: Należy dążyć do tego, aby jak najwięcej prostych zapytań było rozwiązywanych automatycznie. Analiza interakcji zakończonych transferem do konsultanta może wskazać słabe punkty w konfiguracji bota/IVR.
    
5.  **Obciążenie w Ciągu Dnia:** Godziny szczytu w obu latach przypadają na [wpisz godziny szczytu, np. 10:00-14:00].
    Rekomendacja: Ta informacja jest krytyczna dla działu Workforce Management. Należy zoptymalizować grafiki pracy konsultantów, aby zapewnić odpowiednią obsadę w godzinach największego ruchu i uniknąć przepracowania zespołu lub długiego czasu oczekiwania dla klientów.

Analiza ta stanowi solidną podstawę do podejmowania dalszych, świadomych decyzji biznesowych w celu optymalizacji pracy infolinii.
""")
print("="*80)



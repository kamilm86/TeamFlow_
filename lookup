To b≈ÇƒÖd programistyczny (nie bazy danych). Kod w funkcji insert_data pr√≥buje odwo≈Çaƒá siƒô do s≈Çownika self.type_mappings, kt√≥rego nie zdefiniowali≈õmy w __init__ Twojej klasy.
Aby to naprawiƒá szybko i bezbole≈õnie (bez szukania metody __init__), zdefiniujemy ten s≈Çownik lokalnie wewnƒÖtrz funkcji insert_data. Dziƒôki temu funkcja bƒôdzie samowystarczalna.
Oto ostateczna, pe≈Çna wersja funkcji insert_data.
Zawiera ona:
 * Naprawƒô b≈Çƒôdu type_mappings (definicja lokalna).
 * Agresywne czyszczenie danych (zamiana przecink√≥w na kropki, usuwanie <NA>, wymuszanie liczb) ‚Äì ≈ºeby uniknƒÖƒá ORA-01722.
 * Plan B (Filtrowanie w RAM) ‚Äì wywo≈Çanie funkcji filtrujƒÖcej zamiast SQL-owego lookupu.
Wklej ten kod w miejsce starej funkcji insert_data:
    def insert_data(self, df: pd.DataFrame, target_db: str, table_name: str, 
                    schema: str, if_exists: str = 'append', batch_size: int = 1000000, 
                    verbose: bool = True, use_fast_executemany: bool = True, 
                    lookup_keys: Optional[List[str]] = None) -> None:
        
        from sqlalchemy import text
        import numpy as np
        import gc

        self.logger.info(f"Rozpoczynanie wstawiania do {target_db}.{schema}.{table_name} (Wierszy: {len(df)})")
        engine = self._get_engine(target_db) 
        
        try:
            with engine.connect() as connection:
                # Ustawienia dla SQL Server (opcjonalne, bezpieczne)
                if target_db == 'SQLServer':
                    try:
                        connection.execute(text("SET LOCK_TIMEOUT 1200000"))
                        connection.execute(text("SET TRANSACTION ISOLATION LEVEL READ COMMITTED"))
                    except Exception:
                        pass # Ignorujemy je≈õli sterownik nie obs≈Çuguje

                # 1. Transformacje DataFrame
                df_to_load = df.copy() 
                df_to_load = self.standardize_column_names(df_to_load) 
                df_to_load = self.handle_nulls(df_to_load)
                
                # Wstƒôpne czyszczenie string√≥w
                df_to_load = df_to_load.astype(str).replace({pd.NaT: None, 'nan': None, '<NA>': None}) 
                
                # --- ROZDZIELENIE TYP√ìW (NAPRAWA B≈ÅƒòDU VARCHAR) ---
                # Typy SQL dla create_table (zwraca obiekty SQLAlchemy)
                sqlalchemy_dtype_dict = self.map_data_types(df_to_load, target_db) 
                
                # üí•üí•üí• NAPRAWA B≈ÅƒòDU type_mappings üí•üí•üí•
                # Definiujemy mapowanie LOKALNIE, ≈ºeby nie musieƒá zmieniaƒá __init__
                local_type_mappings = {
                    'Oracle': {
                        'int64': 'int64',
                        'float64': 'float64',
                        'datetime64[ns]': 'datetime64[ns]',
                        'object': 'str',
                        'bool': 'int64', # Oracle nie ma boola
                        'date': 'datetime64[ns]'
                    },
                    'SQLServer': {'int64': 'int64', 'bool': 'bool', 'object': 'str'},
                    'Impala': {'int64': 'int64', 'bool': 'bool', 'object': 'str'}
                }
                numpy_type_mappings = local_type_mappings.get(target_db, {})

                # Budujemy s≈Çownik typ√≥w dla Pandas astype
                numpy_dtype_dict = {}
                for col_name in df_to_load.columns:
                    dtype = df_to_load[col_name].dtype.name
                    if dtype in numpy_type_mappings:
                        numpy_dtype_dict[col_name] = numpy_type_mappings[dtype]
                    elif 'datetime' in dtype:
                         numpy_dtype_dict[col_name] = 'datetime64[ns]'
                    else:
                        numpy_dtype_dict[col_name] = 'str' 
                
                self.logger.info(f"Konwersja typ√≥w (astype)...") 
                # Tutaj u≈ºywamy bezpiecznych typ√≥w stringowych dla Pandasa
                df_to_load = df_to_load.astype(numpy_dtype_dict) 
                
                # --- AGRESYWNE CZYSZCZENIE (NAPRAWA B≈ÅƒòDU ORA-01722) ---
                
                # Lista kolumn znanych jako liczbowe (z Twoich log√≥w b≈Çƒôd√≥w)
                # Mo≈ºesz tu dopisaƒá inne kolumny, kt√≥re sprawiajƒÖ problemy
                known_numeric_cols = [
                    'TIMEINIVR', 'CALLDURATION', 'ILE_INT', 'SEGMENTID_START', 
                    'SEGMENTID_END', 'LASTASSIGNEDWORKGROUPID'
                ]
                
                for col in df_to_load.columns:
                    # Sprawdzamy czy kolumna powinna byƒá liczbowa (po nazwie lub typie)
                    is_numeric = (col in known_numeric_cols) or \
                                 ('int' in str(numpy_dtype_dict.get(col, ''))) or \
                                 ('float' in str(numpy_dtype_dict.get(col, '')))

                    if is_numeric:
                        # 1. Najpierw na string, ≈ºeby m√≥c czy≈õciƒá
                        df_to_load[col] = df_to_load[col].astype(str)
                        
                        # 2. Zamiana typowych ≈õmieci na NaN
                        replace_dict = {'<NA>': np.nan, 'nan': np.nan, 'NaN': np.nan, 'None': np.nan, '': np.nan}
                        for k, v in replace_dict.items():
                             df_to_load[col] = df_to_load[col].replace(k, v)
                        
                        # 3. Zamiana przecinka na kropkƒô (kluczowe dla PL danych)
                        df_to_load[col] = df_to_load[col].str.replace(',', '.', regex=False)
                        
                        # 4. Wymuszenie konwersji na liczbƒô (b≈Çƒôdy zamieniane na NaN)
                        df_to_load[col] = pd.to_numeric(df_to_load[col], errors='coerce')

                # Zamiana wszystkich NaN na None (SQL NULL) - to jest bezpieczne dla bazy
                df_to_load = df_to_load.where(pd.notnull(df_to_load), None)
                self.logger.info("Dane wyczyszczone (liczby naprawione, NaN -> NULL).")
                
                # 2. Tworzenie tabeli (je≈õli trzeba)
                if not self.create_table_if_not_exists(engine, sqlalchemy_dtype_dict, df_to_load, table_name, schema, target_db):
                     if if_exists == 'replace':
                         connection.execute(text(f'DROP TABLE "{schema}"."{table_name}"'))
                         self.create_table_if_not_exists(engine, sqlalchemy_dtype_dict, df_to_load, table_name, schema, target_db)
                     elif if_exists == 'truncate':
                         connection.execute(text(f'TRUNCATE TABLE "{schema}"."{table_name}"'))
                
                # Sprawdzenie rozmiar√≥w kolumn (ignoruje b≈Çƒôdy ORA-01439)
                self.check_column_size(engine, df_to_load, table_name, schema, target_db)
                
                # 3. Batching i Insert (Plan B - Filtrowanie w RAM)
                total_rows = len(df_to_load)
                if total_rows == 0:
                    return

                full_batches = total_rows // batch_size
                last_batch_size = total_rows % batch_size
                
                # Przetwarzanie pe≈Çnych partii
                for batch_num in range(full_batches):
                    start = batch_num * batch_size
                    end = start + batch_size
                    batch = df_to_load.iloc[start:end].copy()
                    
                    if verbose:
                        self.logger.info(f"Przetwarzanie partii {batch_num+1}/{full_batches+1}...")

                    # Wywo≈Çanie funkcji z Planu B (musi byƒá zaimplementowana jak poda≈Çem wcze≈õniej)
                    self._custom_merge_insert(
                        target_db=target_db,
                        batch=batch,
                        table=table_name,
                        schema=schema,
                        lookup_keys=lookup_keys, 
                        conn=connection 
                    )
                    del batch
                    gc.collect()

                # Ostatnia partia
                if last_batch_size > 0:
                    start = full_batches * batch_size
                    last_batch = df_to_load.iloc[start:total_rows].copy()
                    self.logger.info(f"Przetwarzanie ostatniej partii...")
                    
                    self._custom_merge_insert(
                        target_db=target_db,
                        batch=last_batch,
                        table=table_name,
                        schema=schema,
                        lookup_keys=lookup_keys, 
                        conn=connection
                    )
                    del last_batch
                    gc.collect()
                
                self.logger.info(f"Proces zako≈Ñczony dla {table_name}.")

        except Exception as e:
            self.logger.error(f"Krytyczny b≈ÇƒÖd w metodzie insert_data: {e}")
            raise
        finally:
            if 'engine' in locals():
                pass 

Co sprawdziƒá po wklejeniu?
Upewnij siƒô, ≈ºe masz wklejonƒÖ r√≥wnie≈º funkcjƒô _custom_merge_insert w wersji Plan B (tƒô z filtrowaniem w RAM, kt√≥rƒÖ poda≈Çem w poprzedniej wiadomo≈õci).
Ta kombinacja (lokalna definicja typ√≥w + agresywne czyszczenie + filtrowanie w RAM) powinna byƒá ostatecznym rozwiƒÖzaniem Twoich problem√≥w.
